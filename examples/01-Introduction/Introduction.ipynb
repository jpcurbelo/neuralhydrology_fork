{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NeuralHydrology\n",
    "\n",
    "**Before we start**\n",
    "\n",
    "- This tutorial is rendered from a Jupyter notebook that is hosted on GitHub. If you want to run the code yourself, you can find the notebook and configuration files [here](https://github.com/neuralhydrology/neuralhydrology/tree/master/examples/01-Introduction).\n",
    "- To be able to run this notebook locally, you need to download the publicly available CAMELS US rainfall-runoff dataset. See the [Data Prerequisites Tutorial](data-prerequisites.nblink) for a detailed description on where to download the data and how to structure your local dataset folder. You will also need to follow the [installation instructions](https://neuralhydrology.readthedocs.io/en/latest/usage/quickstart.html#installation) (the easiest option if you don't plan to implement your own models/datasets is `pip install neuralhydrology`; for other options refer to the installation instructions).\n",
    "\n",
    "The Python package NeuralHydrology was was developed with a strong focus on research. The main application area is hydrology, however, in principle the code can be used with any data. To allow fast iteration of research ideas, we tried to develop the package as modular as possible so that new models, new data sets, new loss functions, new regularizations, new metrics etc. can be integrated with minor effort.\n",
    "\n",
    "There are two different ways to use this package:\n",
    "\n",
    "1. From the terminal, making use of some high-level entry points (such as `nh-run` and `nh-run-scheduler`)\n",
    "2. From any other Python file or Jupyter Notebook, using NeuralHydrology's API\n",
    "\n",
    "In this tutorial, we will give a very short overview of the two different modes.\n",
    "\n",
    "Both approaches require a **configuration file**. These are `.yml` files which define the entire run configuration (such as data set, basins, data periods, model specifications, etc.). A full list of config arguments is listed in the [documentation](https://neuralhydrology.readthedocs.io/en/latest/usage/config.html) and we highly recommend to check this page and read the documentation carefully. There is a lot that you can do with this Python package and we can't cover everything in tutorials.\n",
    "\n",
    "For every run that you start, a new folder will be created. This folder is used to store the model and optimizer checkpoints, train data means/stds (needed for scaling during inference), tensorboard log file (can be used to monitor and compare training runs visually), validation results (optionally) and training progress figures (optionally, e.g., model predictions and observations for _n_ random basins). During inference, the evaluation results will also be stored in this directory (e.g., test period results).\n",
    "\n",
    "\n",
    "### TensorBoard logging\n",
    "By default, the training progress is logged in TensorBoard files (add `log_tensorboard: False` to the config to disable TensorBoard logging). If you installed a Python environment from one of our environment files, you have TensorBoard already installed. If not, you can install TensorBoard with:\n",
    "\n",
    "```\n",
    "pip install tensorboard\n",
    "``` \n",
    "\n",
    "To start the TensorBoard dashboard, run:\n",
    "\n",
    "```\n",
    "tensorboard --logdir /path/to/run-dir\n",
    "```\n",
    "\n",
    "You can also visualize multiple runs at once if you point the `--logdir` to the parent directory (useful for model intercomparison)\n",
    "\n",
    "### File logging\n",
    "In addition to TensorBoard, you will always find a file called `output.log` in the run directory. This file is a dump of the console output you see during training and evaluation.\n",
    "\n",
    "\n",
    "## Using NeuralHydrology from the Terminal\n",
    "\n",
    "### nh-run\n",
    "\n",
    "\n",
    "Given a run configuration file, you can use the bash command `nh-run` to train/evaluate a model. To train a model, use\n",
    "\n",
    "\n",
    "```bash\n",
    "nh-run train --config-file path/to/config.yml\n",
    "```\n",
    "\n",
    "to evaluate the model after training, use\n",
    "\n",
    "```bash\n",
    "nh-run evaluate --run-dir path/to/run-directory\n",
    "```\n",
    "\n",
    "### nh-run-scheduler\n",
    "\n",
    "If you want to train/evaluate multiple models on different GPUs, you can use the `nh-run-scheduler`. This tool automatically distributes runs across GPUs and starts a new one, whenever one run finishes.\n",
    "\n",
    "Calling `nh-run-scheduler` in `train` mode will train one model for each `.yml` file in a directory (or its sub-directories).\n",
    "\n",
    "```bash\n",
    "nh-run-scheduler train --directory /path/to/config-dir --runs-per-gpu 2 --gpu_ids 0 1 2 3 \n",
    "```\n",
    "Use `-runs-per-gpu` to define the number of models that are simultaneously trained on a _single_ GPU (2 in this case) and `--gpu-ids` to define which GPUs will be used (numbers are ids according to nvidia-smi). In this example, 8 models will train simultaneously on 4 different GPUs.\n",
    "\n",
    "Calling `nh-run-scheduler` in `evaluate` mode will evaluate all models in all run directories in a given root directory.\n",
    "\n",
    "```bash\n",
    "nh-run-scheduler evaluate --directory /path/to/parent-run-dir/ --runs-per-gpu 2 --gpu_ids 0 1 2 3 \n",
    "```\n",
    "\n",
    "## API usage\n",
    "\n",
    "Besides the command line tools, you can also use the NeuralHydrology package just like any other Python package by importing its modules, classes, or functions.\n",
    "\n",
    "This can be helpful for exploratory studies with trained models, but also if you want to use some of the functions or classes within a different codebase. \n",
    "\n",
    "Look at the [API Documentation](https://neuralhydrology.readthedocs.io/en/latest/api/neuralhydrology.html) for a full list of functions/classes you could use.\n",
    "\n",
    "The following example shows how to train and evaluate a model via the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ame805/neuralhydrology_fork/venv-nh/lib/python3.8/site-packages/nvidia/cudnn/lib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from neuralhydrology.evaluation import metrics\n",
    "from neuralhydrology.nh_run import start_run, eval_run, continue_run\n",
    "\n",
    "import os\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/home/ame805/neuralhydrology_fork/venv-nh/lib/python3.8/site-packages/nvidia/cudnn/lib'\n",
    "# os.environ['LD_LIBRARY_PATH'] = '../.././venv-nh/lib/python3.8/site-packages/nvidia/cudnn/lib'\n",
    "print(os.environ['LD_LIBRARY_PATH'])\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and format it as 'ddmm' (e.g., '0711' for November 7th)\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "# current_date = '2024-03-04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.1.0+cu121\n",
      "Cuda Version:  12.1\n",
      "CudaNN Version:  8902\n"
     ]
    }
   ],
   "source": [
    "# Print torch version\n",
    "print(\"Torch Version: \", torch.__version__)\n",
    "# Print cuda version\n",
    "print(\"Cuda Version: \", torch.version.cuda)\n",
    "# Print cudaNN version\n",
    "print(\"CudaNN Version: \", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model for a single config file\n",
    "\n",
    "**Note**\n",
    "\n",
    "- The config file assumes that the CAMELS US dataset is stored under `data/CAMELS_US` (relative to the main directory of this repository) or a symbolic link exists at this location. Make sure that this folder contains the required subdirectories `basin_mean_forcing`, `usgs_streamflow` and `camels_attributes_v2.0`. If your data is stored at a different location and you can't or don't want to create a symbolic link, you will need to change the `data_dir` argument in the `1_basin.yml` config file that is located in the same directory as this notebook.\n",
    "- By default, the config (`1_basin.yml`) assumes that you have a CUDA-capable NVIDIA GPU (see config argument `device`). In case you don't have any or you have one but want to train on the CPU, you can either change the config argument to `device: cpu` or pass `gpu=-1` to the `start_run()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # os.listdir('../../../../../gladwell/hydrology/SUMMA/summa-ml-models/CAMELS_spat_NH/CAMELS_spat_CAN/CAN_01AD002.csv')\n",
    "# list_dir = os.listdir('../../../../../gladwell/hydrology/SUMMA/summa-ml-models/CAMELS_spat_NH/CAMELS_spat_CAN')\n",
    "# print(len(list_dir), 'CAN_01AD002.csv' in list_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-15 15:56:14,938: Logging to /home/ame805/neuralhydrology_fork/examples/01-Introduction/runs/test_run_1507_155614/output.log initialized.\n",
      "2024-07-15 15:56:14,939: ### Folder structure created at /home/ame805/neuralhydrology_fork/examples/01-Introduction/runs/test_run_1507_155614\n",
      "2024-07-15 15:56:14,940: ### Run configurations for test_run\n",
      "2024-07-15 15:56:14,940: experiment_name: test_run\n",
      "2024-07-15 15:56:14,941: train_basin_file: 1_basinB.txt\n",
      "2024-07-15 15:56:14,941: validation_basin_file: 1_basinB.txt\n",
      "2024-07-15 15:56:14,942: test_basin_file: 1_basinB.txt\n",
      "2024-07-15 15:56:14,942: train_start_date: 1980-10-01 00:00:00\n",
      "2024-07-15 15:56:14,943: train_end_date: 2000-09-30 00:00:00\n",
      "2024-07-15 15:56:14,944: validation_start_date: 2000-10-01 00:00:00\n",
      "2024-07-15 15:56:14,945: validation_end_date: 2010-09-30 00:00:00\n",
      "2024-07-15 15:56:14,946: test_start_date: 2000-10-01 00:00:00\n",
      "2024-07-15 15:56:14,947: test_end_date: 2010-09-30 00:00:00\n",
      "2024-07-15 15:56:14,947: device: cuda:0\n",
      "2024-07-15 15:56:14,948: validate_every: 3\n",
      "2024-07-15 15:56:14,948: validate_n_random_basins: 1\n",
      "2024-07-15 15:56:14,948: metrics: ['NSE']\n",
      "2024-07-15 15:56:14,949: model: cudalstm\n",
      "2024-07-15 15:56:14,949: head: regression\n",
      "2024-07-15 15:56:14,950: output_activation: linear\n",
      "2024-07-15 15:56:14,950: hidden_size: 64\n",
      "2024-07-15 15:56:14,951: initial_forget_bias: 3\n",
      "2024-07-15 15:56:14,951: output_dropout: 0.4\n",
      "2024-07-15 15:56:14,952: optimizer: Adam\n",
      "2024-07-15 15:56:14,952: loss: MSE\n",
      "2024-07-15 15:56:14,953: learning_rate: {0: 0.01, 30: 0.005, 40: 0.001}\n",
      "2024-07-15 15:56:14,954: batch_size: 256\n",
      "2024-07-15 15:56:14,954: epochs: 50\n",
      "2024-07-15 15:56:14,955: clip_gradient_norm: 1\n",
      "2024-07-15 15:56:14,955: predict_last_n: 1\n",
      "2024-07-15 15:56:14,956: seq_length: 365\n",
      "2024-07-15 15:56:14,957: num_workers: 8\n",
      "2024-07-15 15:56:14,957: log_interval: 5\n",
      "2024-07-15 15:56:14,958: log_tensorboard: True\n",
      "2024-07-15 15:56:14,958: log_n_figures: 1\n",
      "2024-07-15 15:56:14,959: save_weights_every: 1\n",
      "2024-07-15 15:56:14,959: dataset: camels_us\n",
      "2024-07-15 15:56:14,960: data_dir: ../../../../../gladwell/hydrology/SUMMA/summa-ml-models/CAMELS_US\n",
      "2024-07-15 15:56:14,961: forcings: ['daymet']\n",
      "2024-07-15 15:56:14,961: dynamic_inputs: ['prcp(mm/day)', 'tmin(C)', 'tmax(C)', 'srad(W/m2)', 'vp(Pa)']\n",
      "2024-07-15 15:56:14,962: target_variables: ['QObs(mm/d)']\n",
      "2024-07-15 15:56:14,962: clip_targets_to_zero: ['QObs(mm/d)']\n",
      "2024-07-15 15:56:14,963: number_of_basins: 1\n",
      "2024-07-15 15:56:14,963: run_dir: /home/ame805/neuralhydrology_fork/examples/01-Introduction/runs/test_run_1507_155614\n",
      "2024-07-15 15:56:14,964: train_dir: /home/ame805/neuralhydrology_fork/examples/01-Introduction/runs/test_run_1507_155614/train_data\n",
      "2024-07-15 15:56:14,965: img_log_dir: /home/ame805/neuralhydrology_fork/examples/01-Introduction/runs/test_run_1507_155614/img_log\n",
      "2024-07-15 15:56:14,999: ### Device cuda:0 will be used for training\n",
      "2024-07-15 15:56:15,000: Loading basin data into xarray data set.\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n",
      "2024-07-15 15:56:15,295: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.94s/it]\n",
      "# Epoch 1: 100%|██████████| 29/29 [00:04<00:00,  7.22it/s, Loss: 0.1368]\n",
      "2024-07-15 15:56:24,843: Epoch 1 average loss: avg_loss: 0.42046, avg_total_loss: 0.42046\n",
      "# Epoch 2: 100%|██████████| 29/29 [00:01<00:00, 19.57it/s, Loss: 0.0858]\n",
      "2024-07-15 15:56:26,336: Epoch 2 average loss: avg_loss: 0.31392, avg_total_loss: 0.31392\n",
      "# Epoch 3: 100%|██████████| 29/29 [00:01<00:00, 28.36it/s, Loss: 0.1086]\n",
      "2024-07-15 15:56:27,371: Epoch 3 average loss: avg_loss: 0.26106, avg_total_loss: 0.26106\n",
      "# Validation: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
      "2024-07-15 15:56:29,977: Epoch 3 average validation loss: 0.12009 -- Median validation metrics: avg_loss: 0.12009, NSE: 0.34735\n",
      "# Epoch 4: 100%|██████████| 29/29 [00:01<00:00, 19.46it/s, Loss: 0.1001]\n",
      "2024-07-15 15:56:31,473: Epoch 4 average loss: avg_loss: 0.20342, avg_total_loss: 0.20342\n",
      "# Epoch 5: 100%|██████████| 29/29 [00:01<00:00, 22.57it/s, Loss: 0.0842]\n",
      "2024-07-15 15:56:32,767: Epoch 5 average loss: avg_loss: 0.18895, avg_total_loss: 0.18895\n",
      "# Epoch 6: 100%|██████████| 29/29 [00:01<00:00, 19.29it/s, Loss: 0.1131]\n",
      "2024-07-15 15:56:34,281: Epoch 6 average loss: avg_loss: 0.21908, avg_total_loss: 0.21908\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "2024-07-15 15:56:35,132: Epoch 6 average validation loss: 0.11107 -- Median validation metrics: avg_loss: 0.11107, NSE: 0.36207\n",
      "# Epoch 7: 100%|██████████| 29/29 [00:01<00:00, 24.40it/s, Loss: 0.1577]\n",
      "2024-07-15 15:56:36,327: Epoch 7 average loss: avg_loss: 0.13262, avg_total_loss: 0.13262\n",
      "# Epoch 8: 100%|██████████| 29/29 [00:01<00:00, 21.16it/s, Loss: 0.0573]\n",
      "2024-07-15 15:56:37,713: Epoch 8 average loss: avg_loss: 0.12784, avg_total_loss: 0.12784\n",
      "# Epoch 9: 100%|██████████| 29/29 [00:01<00:00, 24.31it/s, Loss: 0.0632]\n",
      "2024-07-15 15:56:38,925: Epoch 9 average loss: avg_loss: 0.09861, avg_total_loss: 0.09861\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\n",
      "2024-07-15 15:56:39,860: Epoch 9 average validation loss: 0.10283 -- Median validation metrics: avg_loss: 0.10283, NSE: 0.39967\n",
      "# Epoch 10: 100%|██████████| 29/29 [00:01<00:00, 19.17it/s, Loss: 0.0667]\n",
      "2024-07-15 15:56:41,385: Epoch 10 average loss: avg_loss: 0.08453, avg_total_loss: 0.08453\n",
      "# Epoch 11: 100%|██████████| 29/29 [00:01<00:00, 20.11it/s, Loss: 0.0613]\n",
      "2024-07-15 15:56:42,852: Epoch 11 average loss: avg_loss: 0.07582, avg_total_loss: 0.07582\n",
      "# Epoch 12: 100%|██████████| 29/29 [00:01<00:00, 20.55it/s, Loss: 0.0947]\n",
      "2024-07-15 15:56:44,275: Epoch 12 average loss: avg_loss: 0.07274, avg_total_loss: 0.07274\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "2024-07-15 15:56:45,489: Epoch 12 average validation loss: 0.15094 -- Median validation metrics: avg_loss: 0.15094, NSE: 0.11466\n",
      "# Epoch 13: 100%|██████████| 29/29 [00:01<00:00, 21.23it/s, Loss: 0.0654]\n",
      "2024-07-15 15:56:46,861: Epoch 13 average loss: avg_loss: 0.13564, avg_total_loss: 0.13564\n",
      "# Epoch 14: 100%|██████████| 29/29 [00:01<00:00, 19.65it/s, Loss: 0.0616]\n",
      "2024-07-15 15:56:48,347: Epoch 14 average loss: avg_loss: 0.08274, avg_total_loss: 0.08274\n",
      "# Epoch 15: 100%|██████████| 29/29 [00:01<00:00, 21.55it/s, Loss: 0.0310]\n",
      "2024-07-15 15:56:49,704: Epoch 15 average loss: avg_loss: 0.08882, avg_total_loss: 0.08882\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "2024-07-15 15:56:50,538: Epoch 15 average validation loss: 0.13846 -- Median validation metrics: avg_loss: 0.13846, NSE: 0.18733\n",
      "# Epoch 16: 100%|██████████| 29/29 [00:01<00:00, 22.14it/s, Loss: 0.0228]\n",
      "2024-07-15 15:56:51,856: Epoch 16 average loss: avg_loss: 0.07609, avg_total_loss: 0.07609\n",
      "# Epoch 17: 100%|██████████| 29/29 [00:01<00:00, 22.16it/s, Loss: 0.0309]\n",
      "2024-07-15 15:56:53,178: Epoch 17 average loss: avg_loss: 0.07641, avg_total_loss: 0.07641\n",
      "# Epoch 18: 100%|██████████| 29/29 [00:01<00:00, 20.00it/s, Loss: 0.0216]\n",
      "2024-07-15 15:56:54,649: Epoch 18 average loss: avg_loss: 0.07508, avg_total_loss: 0.07508\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "2024-07-15 15:56:55,790: Epoch 18 average validation loss: 0.12965 -- Median validation metrics: avg_loss: 0.12965, NSE: 0.23687\n",
      "# Epoch 19: 100%|██████████| 29/29 [00:01<00:00, 24.02it/s, Loss: 0.0297]\n",
      "2024-07-15 15:56:57,004: Epoch 19 average loss: avg_loss: 0.06156, avg_total_loss: 0.06156\n",
      "# Epoch 20: 100%|██████████| 29/29 [00:01<00:00, 19.30it/s, Loss: 0.0256]\n",
      "2024-07-15 15:56:58,518: Epoch 20 average loss: avg_loss: 0.04930, avg_total_loss: 0.04930\n",
      "# Epoch 21: 100%|██████████| 29/29 [00:01<00:00, 25.78it/s, Loss: 0.0513]\n",
      "2024-07-15 15:56:59,663: Epoch 21 average loss: avg_loss: 0.04831, avg_total_loss: 0.04831\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "2024-07-15 15:57:00,626: Epoch 21 average validation loss: 0.10261 -- Median validation metrics: avg_loss: 0.10261, NSE: 0.39799\n",
      "# Epoch 22: 100%|██████████| 29/29 [00:01<00:00, 19.65it/s, Loss: 0.0671]\n",
      "2024-07-15 15:57:02,108: Epoch 22 average loss: avg_loss: 0.05080, avg_total_loss: 0.05080\n",
      "# Epoch 23: 100%|██████████| 29/29 [00:01<00:00, 18.40it/s, Loss: 0.0279]\n",
      "2024-07-15 15:57:03,696: Epoch 23 average loss: avg_loss: 0.04825, avg_total_loss: 0.04825\n",
      "# Epoch 24: 100%|██████████| 29/29 [00:01<00:00, 19.01it/s, Loss: 0.0160]\n",
      "2024-07-15 15:57:05,231: Epoch 24 average loss: avg_loss: 0.04390, avg_total_loss: 0.04390\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\n",
      "2024-07-15 15:57:06,475: Epoch 24 average validation loss: 0.12481 -- Median validation metrics: avg_loss: 0.12481, NSE: 0.26263\n",
      "# Epoch 25: 100%|██████████| 29/29 [00:01<00:00, 19.13it/s, Loss: 0.0284]\n",
      "2024-07-15 15:57:07,997: Epoch 25 average loss: avg_loss: 0.04260, avg_total_loss: 0.04260\n",
      "# Epoch 26: 100%|██████████| 29/29 [00:01<00:00, 19.22it/s, Loss: 0.0144]\n",
      "2024-07-15 15:57:09,517: Epoch 26 average loss: avg_loss: 0.04725, avg_total_loss: 0.04725\n",
      "# Epoch 27: 100%|██████████| 29/29 [00:01<00:00, 18.02it/s, Loss: 0.0111]\n",
      "2024-07-15 15:57:11,137: Epoch 27 average loss: avg_loss: 0.03607, avg_total_loss: 0.03607\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]\n",
      "2024-07-15 15:57:12,061: Epoch 27 average validation loss: 0.10800 -- Median validation metrics: avg_loss: 0.10800, NSE: 0.36323\n",
      "# Epoch 28: 100%|██████████| 29/29 [00:01<00:00, 21.26it/s, Loss: 0.0200]\n",
      "2024-07-15 15:57:13,430: Epoch 28 average loss: avg_loss: 0.03709, avg_total_loss: 0.03709\n",
      "# Epoch 29: 100%|██████████| 29/29 [00:01<00:00, 19.66it/s, Loss: 0.0223]\n",
      "2024-07-15 15:57:14,915: Epoch 29 average loss: avg_loss: 0.04659, avg_total_loss: 0.04659\n",
      "2024-07-15 15:57:14,921: Setting learning rate to 0.005\n",
      "# Epoch 30: 100%|██████████| 29/29 [00:01<00:00, 19.36it/s, Loss: 0.0171]\n",
      "2024-07-15 15:57:16,423: Epoch 30 average loss: avg_loss: 0.03519, avg_total_loss: 0.03519\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "2024-07-15 15:57:17,573: Epoch 30 average validation loss: 0.12135 -- Median validation metrics: avg_loss: 0.12135, NSE: 0.27876\n",
      "# Epoch 31: 100%|██████████| 29/29 [00:01<00:00, 22.48it/s, Loss: 0.0158]\n",
      "2024-07-15 15:57:18,871: Epoch 31 average loss: avg_loss: 0.02905, avg_total_loss: 0.02905\n",
      "# Epoch 32: 100%|██████████| 29/29 [00:01<00:00, 20.93it/s, Loss: 0.0288]\n",
      "2024-07-15 15:57:20,268: Epoch 32 average loss: avg_loss: 0.02829, avg_total_loss: 0.02829\n",
      "# Epoch 33: 100%|██████████| 29/29 [00:01<00:00, 17.56it/s, Loss: 0.0131]\n",
      "2024-07-15 15:57:21,928: Epoch 33 average loss: avg_loss: 0.03483, avg_total_loss: 0.03483\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "2024-07-15 15:57:23,285: Epoch 33 average validation loss: 0.11211 -- Median validation metrics: avg_loss: 0.11211, NSE: 0.33763\n",
      "# Epoch 34: 100%|██████████| 29/29 [00:01<00:00, 23.22it/s, Loss: 0.0146]\n",
      "2024-07-15 15:57:24,543: Epoch 34 average loss: avg_loss: 0.03022, avg_total_loss: 0.03022\n",
      "# Epoch 35: 100%|██████████| 29/29 [00:01<00:00, 20.84it/s, Loss: 0.1636]\n",
      "2024-07-15 15:57:25,947: Epoch 35 average loss: avg_loss: 0.03477, avg_total_loss: 0.03477\n",
      "# Epoch 36: 100%|██████████| 29/29 [00:01<00:00, 18.14it/s, Loss: 0.0116]\n",
      "2024-07-15 15:57:27,555: Epoch 36 average loss: avg_loss: 0.02760, avg_total_loss: 0.02760\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "2024-07-15 15:57:28,609: Epoch 36 average validation loss: 0.12282 -- Median validation metrics: avg_loss: 0.12282, NSE: 0.27179\n",
      "# Epoch 37: 100%|██████████| 29/29 [00:01<00:00, 23.34it/s, Loss: 0.0114]\n",
      "2024-07-15 15:57:29,866: Epoch 37 average loss: avg_loss: 0.03547, avg_total_loss: 0.03547\n",
      "# Epoch 38: 100%|██████████| 29/29 [00:01<00:00, 20.85it/s, Loss: 0.0119]\n",
      "2024-07-15 15:57:31,272: Epoch 38 average loss: avg_loss: 0.02201, avg_total_loss: 0.02201\n",
      "# Epoch 39: 100%|██████████| 29/29 [00:01<00:00, 18.67it/s, Loss: 0.0140]\n",
      "2024-07-15 15:57:32,839: Epoch 39 average loss: avg_loss: 0.02212, avg_total_loss: 0.02212\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "2024-07-15 15:57:33,796: Epoch 39 average validation loss: 0.12203 -- Median validation metrics: avg_loss: 0.12203, NSE: 0.27566\n",
      "2024-07-15 15:57:33,799: Setting learning rate to 0.001\n",
      "# Epoch 40: 100%|██████████| 29/29 [00:01<00:00, 24.20it/s, Loss: 0.0198]\n",
      "2024-07-15 15:57:35,004: Epoch 40 average loss: avg_loss: 0.02531, avg_total_loss: 0.02531\n",
      "# Epoch 41: 100%|██████████| 29/29 [00:01<00:00, 24.65it/s, Loss: 0.0190]\n",
      "2024-07-15 15:57:36,193: Epoch 41 average loss: avg_loss: 0.02688, avg_total_loss: 0.02688\n",
      "# Epoch 42: 100%|██████████| 29/29 [00:01<00:00, 19.23it/s, Loss: 0.0115]\n",
      "2024-07-15 15:57:37,711: Epoch 42 average loss: avg_loss: 0.02482, avg_total_loss: 0.02482\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "2024-07-15 15:57:38,776: Epoch 42 average validation loss: 0.12025 -- Median validation metrics: avg_loss: 0.12025, NSE: 0.28594\n",
      "# Epoch 43: 100%|██████████| 29/29 [00:01<00:00, 17.72it/s, Loss: 0.0131]\n",
      "2024-07-15 15:57:40,431: Epoch 43 average loss: avg_loss: 0.02626, avg_total_loss: 0.02626\n",
      "# Epoch 44: 100%|██████████| 29/29 [00:01<00:00, 18.17it/s, Loss: 0.0122]\n",
      "2024-07-15 15:57:42,037: Epoch 44 average loss: avg_loss: 0.01831, avg_total_loss: 0.01831\n",
      "# Epoch 45: 100%|██████████| 29/29 [00:01<00:00, 19.93it/s, Loss: 0.0165]\n",
      "2024-07-15 15:57:43,502: Epoch 45 average loss: avg_loss: 0.02123, avg_total_loss: 0.02123\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n",
      "2024-07-15 15:57:44,375: Epoch 45 average validation loss: 0.12172 -- Median validation metrics: avg_loss: 0.12172, NSE: 0.27927\n",
      "# Epoch 46: 100%|██████████| 29/29 [00:01<00:00, 19.50it/s, Loss: 0.0108]\n",
      "2024-07-15 15:57:45,867: Epoch 46 average loss: avg_loss: 0.02080, avg_total_loss: 0.02080\n",
      "# Epoch 47: 100%|██████████| 29/29 [00:01<00:00, 16.87it/s, Loss: 0.0157]\n",
      "2024-07-15 15:57:47,597: Epoch 47 average loss: avg_loss: 0.01878, avg_total_loss: 0.01878\n",
      "# Epoch 48: 100%|██████████| 29/29 [00:01<00:00, 18.85it/s, Loss: 0.0111]\n",
      "2024-07-15 15:57:49,144: Epoch 48 average loss: avg_loss: 0.02334, avg_total_loss: 0.02334\n",
      "# Validation: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "2024-07-15 15:57:50,053: Epoch 48 average validation loss: 0.12293 -- Median validation metrics: avg_loss: 0.12293, NSE: 0.27072\n",
      "# Epoch 49: 100%|██████████| 29/29 [00:01<00:00, 19.98it/s, Loss: 0.0094]\n",
      "2024-07-15 15:57:51,510: Epoch 49 average loss: avg_loss: 0.02513, avg_total_loss: 0.02513\n",
      "# Epoch 50: 100%|██████████| 29/29 [00:01<00:00, 17.48it/s, Loss: 0.0238]\n",
      "2024-07-15 15:57:53,180: Epoch 50 average loss: avg_loss: 0.02137, avg_total_loss: 0.02137\n"
     ]
    }
   ],
   "source": [
    "basin_file = \"1_basin.yml\"\n",
    "# basin_file = \"10_basin_camels_spat.yml\"\n",
    "# basin_file = \"56_basin_camels_spat.yml\"\n",
    "# basin_file = \"426_basin_camels_spat.yml\"\n",
    "\n",
    "# Load basin_file = \"1_basin.yml\" as a dictionary\n",
    "with open(basin_file, \"r\") as f:\n",
    "    run_config = yaml.safe_load(f)\n",
    "\n",
    "# by default we assume that you have at least one CUDA-capable NVIDIA GPU\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(basin_file))\n",
    "\n",
    "# fall back to CPU-only mode\n",
    "else:\n",
    "    start_run(config_file=Path(basin_file), gpu=-1)\n",
    "\n",
    "# continue_run(run_dir=Path('runs/426_camels_spat_USA_2024-03-05_093239'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate run on test set\n",
    "The run directory that needs to be specified for evaluation is printed in the output log above. Since the folder name is created dynamically (including the date and time of the start of the run) you will need to change the `run_dir` argument according to your local directory name. By default, it will use the same device as during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_run_1507_155614'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Select the last folder in the 'runs' directory that matches the current date\n",
    "# run_files = sorted(os.listdir(\"runs\"))\n",
    "\n",
    "# # Filter filenames that match the current date\n",
    "# exp_name = run_config[\"experiment_name\"]\n",
    "\n",
    "# display(exp_name, current_date)\n",
    "# filtered_run_files = sorted([filename for filename in run_files if filename.replace(f'{exp_name}_', '').split('_')[0] == current_date])\n",
    "\n",
    "# display(filtered_run_files)\n",
    "\n",
    "# # Ensure there are matching filenames for the current date\n",
    "# if filtered_run_files:\n",
    "#     # Sort the filtered filenames by timestamp in descending order\n",
    "#     # The latest filename for the current date is the last in the sorted list\n",
    "#     run_file = filtered_run_files[-1]\n",
    "\n",
    "run_file = 'test_run_1507_155614'\n",
    "\n",
    "run_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-15 15:59:53,463: Using the model weights from runs/test_run_1507_155614/model_epoch050.pt\n",
      "# Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "2024-07-15 15:59:53,956: Stored metrics at runs/test_run_1507_155614/test/model_epoch050/test_metrics_seed268159.csv\n",
      "2024-07-15 15:59:53,958: Stored results at runs/test_run_1507_155614/test/model_epoch050/test_results.p\n"
     ]
    }
   ],
   "source": [
    "run_dir = Path(f\"runs/{run_file}\")\n",
    "eval_run(run_dir=run_dir, period=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect model predictions\n",
    "Next, we load the results file and compare the model predictions with observations. The results file is always a pickled dictionary with one key per basin (even for a single basin). The next-lower dictionary level is the temporal resolution of the predictions. In this case, we trained a model only on daily data ('1D'). Within the temporal resolution, the next-lower dictionary level are `xr`(an xarray Dataset that contains observations and predictions), as well as one key for each metric that was specified in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['06431500'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_str = str(run_config[\"epochs\"]).rjust(3, \"0\")\n",
    "\n",
    "with open(run_dir / \"test\" / f'{\"model_epoch\" + epoch_str}' / \"test_results.p\", \"rb\") as fp:\n",
    "    results = pickle.load(fp)\n",
    "    \n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data variables in the xarray Dataset are named according to the name of the target variables, with suffix `_obs` for the observations and suffix `_sim` for the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_id_by_key(key, dictionary):\n",
    "    id = 0\n",
    "    for k, v in dictionary.items():\n",
    "        if k == key:\n",
    "            return id\n",
    "        id += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:         (date: 3652, time_step: 1)\n",
       "Coordinates:\n",
       "  * date            (date) datetime64[ns] 2000-10-01 2000-10-02 ... 2010-09-30\n",
       "  * time_step       (time_step) int64 0\n",
       "Data variables:\n",
       "    QObs(mm/d)_obs  (date, time_step) float32 0.4181 0.441 ... 0.4525 0.4525\n",
       "    QObs(mm/d)_sim  (date, time_step) float32 0.3432 0.3395 ... 0.4285 0.438</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-c59bd98b-620d-4e45-a742-5d077ec624c2' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-c59bd98b-620d-4e45-a742-5d077ec624c2' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>date</span>: 3652</li><li><span class='xr-has-index'>time_step</span>: 1</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-f45e3bd9-d76e-4c1b-978c-3608f50d8c67' class='xr-section-summary-in' type='checkbox'  checked><label for='section-f45e3bd9-d76e-4c1b-978c-3608f50d8c67' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>date</span></div><div class='xr-var-dims'>(date)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2000-10-01 ... 2010-09-30</div><input id='attrs-53c6537c-0a90-4d54-90ae-50c724951be6' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-53c6537c-0a90-4d54-90ae-50c724951be6' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-64b3a5de-b65a-4999-83ff-d954c54b12de' class='xr-var-data-in' type='checkbox'><label for='data-64b3a5de-b65a-4999-83ff-d954c54b12de' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2000-10-01T00:00:00.000000000&#x27;, &#x27;2000-10-02T00:00:00.000000000&#x27;,\n",
       "       &#x27;2000-10-03T00:00:00.000000000&#x27;, ..., &#x27;2010-09-28T00:00:00.000000000&#x27;,\n",
       "       &#x27;2010-09-29T00:00:00.000000000&#x27;, &#x27;2010-09-30T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time_step</span></div><div class='xr-var-dims'>(time_step)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-4d266d65-0a42-4384-afd0-03b8a8a2e50e' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-4d266d65-0a42-4384-afd0-03b8a8a2e50e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-720149f4-0307-4cae-8bca-5b180d3b30d7' class='xr-var-data-in' type='checkbox'><label for='data-720149f4-0307-4cae-8bca-5b180d3b30d7' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-dc3e1f0b-7c20-4bd6-bccb-3866b2592b54' class='xr-section-summary-in' type='checkbox'  checked><label for='section-dc3e1f0b-7c20-4bd6-bccb-3866b2592b54' class='xr-section-summary' >Data variables: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>QObs(mm/d)_obs</span></div><div class='xr-var-dims'>(date, time_step)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.4181 0.441 ... 0.4525 0.4525</div><input id='attrs-832ecb09-50d9-487b-b83d-40f0c73c6152' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-832ecb09-50d9-487b-b83d-40f0c73c6152' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-01cff1d6-5c7d-47fb-967c-b0cd8bb5cc25' class='xr-var-data-in' type='checkbox'><label for='data-01cff1d6-5c7d-47fb-967c-b0cd8bb5cc25' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[0.4181051 ],\n",
       "       [0.44101498],\n",
       "       [0.4181051 ],\n",
       "       ...,\n",
       "       [0.44674242],\n",
       "       [0.4524699 ],\n",
       "       [0.4524699 ]], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>QObs(mm/d)_sim</span></div><div class='xr-var-dims'>(date, time_step)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.3432 0.3395 ... 0.4285 0.438</div><input id='attrs-a491797e-2e6c-4e78-a76d-584ee6296bf3' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a491797e-2e6c-4e78-a76d-584ee6296bf3' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1e8fc589-d799-40ee-ab80-8ac394063639' class='xr-var-data-in' type='checkbox'><label for='data-1e8fc589-d799-40ee-ab80-8ac394063639' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[0.34321797],\n",
       "       [0.3395013 ],\n",
       "       [0.34188467],\n",
       "       ...,\n",
       "       [0.42070055],\n",
       "       [0.42851174],\n",
       "       [0.43802476]], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-05b15b2d-679d-4984-88d0-786c10c2b6b6' class='xr-section-summary-in' type='checkbox'  ><label for='section-05b15b2d-679d-4984-88d0-786c10c2b6b6' class='xr-section-summary' >Indexes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>date</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-1f808df1-f9cd-4a42-952a-4e5ba0cb3e5a' class='xr-index-data-in' type='checkbox'/><label for='index-1f808df1-f9cd-4a42-952a-4e5ba0cb3e5a' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2000-10-01&#x27;, &#x27;2000-10-02&#x27;, &#x27;2000-10-03&#x27;, &#x27;2000-10-04&#x27;,\n",
       "               &#x27;2000-10-05&#x27;, &#x27;2000-10-06&#x27;, &#x27;2000-10-07&#x27;, &#x27;2000-10-08&#x27;,\n",
       "               &#x27;2000-10-09&#x27;, &#x27;2000-10-10&#x27;,\n",
       "               ...\n",
       "               &#x27;2010-09-21&#x27;, &#x27;2010-09-22&#x27;, &#x27;2010-09-23&#x27;, &#x27;2010-09-24&#x27;,\n",
       "               &#x27;2010-09-25&#x27;, &#x27;2010-09-26&#x27;, &#x27;2010-09-27&#x27;, &#x27;2010-09-28&#x27;,\n",
       "               &#x27;2010-09-29&#x27;, &#x27;2010-09-30&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;date&#x27;, length=3652, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time_step</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-d268531f-a689-4233-80d4-128bdbfa1e87' class='xr-index-data-in' type='checkbox'/><label for='index-d268531f-a689-4233-80d4-128bdbfa1e87' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([0], dtype=&#x27;int64&#x27;, name=&#x27;time_step&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-7e93584c-e79b-42f5-8e66-33f7076d3493' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-7e93584c-e79b-42f5-8e66-33f7076d3493' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:         (date: 3652, time_step: 1)\n",
       "Coordinates:\n",
       "  * date            (date) datetime64[ns] 2000-10-01 2000-10-02 ... 2010-09-30\n",
       "  * time_step       (time_step) int64 0\n",
       "Data variables:\n",
       "    QObs(mm/d)_obs  (date, time_step) float32 0.4181 0.441 ... 0.4525 0.4525\n",
       "    QObs(mm/d)_sim  (date, time_step) float32 0.3432 0.3395 ... 0.4285 0.438"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basin_txt = run_config[\"train_basin_file\"]\n",
    "\n",
    "with open(basin_txt, \"rb\") as fp:\n",
    "    hru_ids_bytes = fp.read()\n",
    "\n",
    "hru_ids = hru_ids_bytes.decode(\"utf-8\").splitlines()\n",
    "\n",
    "HRU2PLOT = 0\n",
    "# HRU2PLOT = find_id_by_key('CAN_08KH001', results)\n",
    "# HRU2PLOT = find_id_by_key('USA_01013500', results)\n",
    "# HRU2PLOT = find_id_by_key('USA_06431500', results)\n",
    "hru_id = hru_ids[HRU2PLOT]\n",
    "\n",
    "results[hru_id]['1D']['xr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model predictions vs. the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'NeuralHydrology - Test period - 06431500 - NSE 0.289')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_var = run_config[\"target_variables\"][0]\n",
    "target_obs = f'{target_var}_obs'\n",
    "target_sim = f'{target_var}_sim'\n",
    "\n",
    "\n",
    "qobs = results[hru_id]['1D']['xr'][target_obs]\n",
    "qsim = results[hru_id]['1D']['xr'][target_sim]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "ax.plot(qobs['date'], qobs, linewidth=3, label='Observed - NH')\n",
    "ax.plot(qsim['date'], qsim, '--', label='Predicted - NH')\n",
    "ax.set_ylabel(\"Discharge (mm/d)\")\n",
    "plt.legend()\n",
    "ax.set_title(f\"NeuralHydrology - Test period - {hru_id} - NSE {results[hru_id]['1D']['NSE']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to compute all metrics that are implemented in the NeuralHydrology package. You will find additional hydrological signatures implemented in `neuralhydrology.evaluation.signatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE: 0.842\n",
      "MSE: 0.998\n",
      "RMSE: 0.999\n",
      "KGE: 0.787\n",
      "Alpha-NSE: 0.831\n",
      "Beta-KGE: 0.891\n",
      "Beta-NSE: -0.103\n",
      "Pearson-r: 0.929\n",
      "FHV: -14.555\n",
      "FMS: -17.064\n",
      "FLV: -14.150\n",
      "Peak-Timing: 0.111\n",
      "Peak-MAPE: 35.818\n"
     ]
    }
   ],
   "source": [
    "values = metrics.calculate_all_metrics(qobs.isel(time_step=-1), qsim.isel(time_step=-1))\n",
    "for key, val in values.items():\n",
    "    print(f\"{key}: {val:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
